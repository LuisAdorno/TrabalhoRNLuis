{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisAdorno/TrabalhoRNLuis/blob/main/Luis_TrabalhoRN1FinalVGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce7f294",
      "metadata": {
        "id": "4ce7f294"
      },
      "source": [
        "#Vamos Classificar RX de covid e normais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88c99b7",
      "metadata": {
        "id": "b88c99b7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c73e9a1",
      "metadata": {
        "id": "5c73e9a1"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fd750e",
      "metadata": {
        "id": "69fd750e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from imutils import paths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc2ff74",
      "metadata": {
        "id": "2dc2ff74"
      },
      "source": [
        "Vamos a procurar o caminho de nossa base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5532b06a",
      "metadata": {
        "id": "5532b06a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "workdir_path = '/content/drive/My Drive'\n",
        "os.chdir(workdir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19a8dd4",
      "metadata": {
        "id": "e19a8dd4"
      },
      "source": [
        "#Vamos colocar os parametros de nosso modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59313b17",
      "metadata": {
        "id": "59313b17"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 1e-3\n",
        "EPOCHS = 15 # PODE MUDAR\n",
        "BS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9390f307",
      "metadata": {
        "id": "9390f307"
      },
      "source": [
        "# Pega a lista de imagens no nosso diret칩rio de conjunto de dados e, em seguida, inicializa\n",
        "# a lista de dados e as respectivas classes das imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0405b8",
      "metadata": {
        "id": "dd0405b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_path = 'dataset'\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset_path))\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c7e6fa",
      "metadata": {
        "id": "64c7e6fa"
      },
      "outputs": [],
      "source": [
        "# recorrer las rutas de la imagen\n",
        "for imagePath in imagePaths:\n",
        "    # extrae la etiqueta de clase del nombre del archivo\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "   # cargue la imagen, intercambie canales de color y cambie su tama침o para que sea una\n",
        "   # 224x224 p칤xeles sin tener en cuenta la relaci칩n de aspecto\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # actualizar las listas de datos y etiquetas, respectivamente\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "print(\"labels: \", np.unique(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c4d1a4",
      "metadata": {
        "id": "d5c4d1a4"
      },
      "outputs": [],
      "source": [
        "# convierta los datos y las etiquetas en matrices NumPy mientras escala el p칤xel\n",
        "# intensidades en el rango [0, 255]\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381406a8",
      "metadata": {
        "id": "381406a8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(data[labels=='covid'][1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd1cdb5",
      "metadata": {
        "id": "3dd1cdb5"
      },
      "outputs": [],
      "source": [
        "# realizar una codificaci칩n one-hot en las etiquetas\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf65ed4",
      "metadata": {
        "id": "acf65ed4"
      },
      "outputs": [],
      "source": [
        "# particione los datos en divisiones de entrenamiento y prueba usando el 80% de\n",
        "# los datos para entrenamiento y el 20% restante para pruebas\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d33fec",
      "metadata": {
        "id": "d1d33fec"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ccc77f",
      "metadata": {
        "id": "a4ccc77f"
      },
      "outputs": [],
      "source": [
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bed6fc7",
      "metadata": {
        "id": "2bed6fc7"
      },
      "outputs": [],
      "source": [
        "# cargue la red VGG16, asegur치ndose de que queden los conjuntos de capas FC principales\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "#headModel = MaxPooling2D(pool_size=(2, 2))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)  # Replace 'decay' with 'learning_rate'\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3549a953",
      "metadata": {
        "id": "3549a953"
      },
      "outputs": [],
      "source": [
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "f1 = baseModel.layers[1].output\n",
        "f2 = baseModel.layers[2].output\n",
        "f3 = baseModel.layers[4].output\n",
        "f4 = baseModel.layers[5].output\n",
        "feature_maps = Model(inputs=baseModel.input, outputs=[f1, f2, f3, f4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386e0316",
      "metadata": {
        "id": "386e0316"
      },
      "outputs": [],
      "source": [
        "y = np.argmax(trainY, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e49b27e",
      "metadata": {
        "id": "2e49b27e"
      },
      "outputs": [],
      "source": [
        "feat1, feat2, feat3, feat4 = feature_maps.predict(trainX[y==1][0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40837031",
      "metadata": {
        "id": "40837031"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axs[0, 0].imshow(feat1[0, :, :, 0:3])\n",
        "axs[0, 1].imshow(feat2[0, :, :, 6:9])\n",
        "axs[1, 0].imshow(feat3[0, :, :, 0:3])\n",
        "axs[1, 1].imshow(feat4[0, :, :, 3:6])\n",
        "# plt.subplots\n",
        "# plt.imshow(feat1[0, :, :, 0:3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd8daf4",
      "metadata": {
        "id": "4dd8daf4"
      },
      "source": [
        "##passo os dados de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6f1a1c",
      "metadata": {
        "id": "3e6f1a1c"
      },
      "outputs": [],
      "source": [
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79a9187",
      "metadata": {
        "id": "d79a9187"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "# plt.savefig(args[\"plot\"])\n",
        "\n",
        "# serialize the model to disk\n",
        "# print(\"[INFO] saving COVID-19 detector model...\")\n",
        "# model.save(args[\"model\"], save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "166926f6",
      "metadata": {
        "id": "166926f6"
      },
      "source": [
        "##Vers칚o modificada com 20 EPOCHS e mais 3 t칠cnicas extras de Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##An치lise e Conclus칚o:\n",
        "O modelo com 15 칠pocas e augmenta칞칚o b치sica parece ter tido desempenho perfeito, mas 칠 poss칤vel que ele tenha sofrido overfitting, aprendendo demais os dados de treino e acertando por sorte em um conjunto pequeno de teste.\n",
        "\n",
        "J치 o modelo com 20 epochs e augmenta칞칚o mais complexa teve desempenho significativamente inferior. A acur치cia caiu para 50%, e tanto a sensibilidade quanto a especificidade foram medianas. Isso pode ter ocorrido por:\n",
        "\n",
        "Excesso de transforma칞칫es visuais, dificultando o aprendizado\n",
        "\n",
        "Aumento da complexidade sem aumento correspondente no tamanho ou diversidade do dataset\n",
        "\n",
        "Mais epochs sem regulariza칞칚o suficiente, causando instabilidade"
      ],
      "metadata": {
        "id": "qzHOBMVFTqoB"
      },
      "id": "qzHOBMVFTqoB"
    },
    {
      "cell_type": "markdown",
      "id": "05d355c9",
      "metadata": {
        "id": "05d355c9"
      },
      "source": [
        "Vamos a procurar o caminho de nossa base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e81df21",
      "metadata": {
        "id": "9e81df21"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "workdir_path = '/content/drive/My Drive'  # Inserir o local da pasta onde est칚o os arquivos de entrada (treino e teste)\n",
        "os.chdir(workdir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f590cde1",
      "metadata": {
        "id": "f590cde1"
      },
      "source": [
        "#Vamos colocar os parametros de nosso modelo (20 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7693b9d9",
      "metadata": {
        "id": "7693b9d9"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 1e-3\n",
        "EPOCHS = 20 # Alterado para 20 epochs\n",
        "BS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe7168d",
      "metadata": {
        "id": "cbe7168d"
      },
      "source": [
        "# Pega a lista de imagens no nosso diret칩rio de conjunto de dados e, em seguida, inicializa\n",
        "# a lista de dados e as respectivas classes das imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e1885",
      "metadata": {
        "id": "dc5e1885"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_path = 'dataset'\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset_path))\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fdc6a1",
      "metadata": {
        "id": "66fdc6a1"
      },
      "outputs": [],
      "source": [
        "# recorrer las rutas de la imagen\n",
        "for imagePath in imagePaths:\n",
        "    # extrae la etiqueta de clase del nombre del archivo\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "   # cargue la imagen, intercambie canales de color y cambie su tama침o para que sea una\n",
        "   # 224x224 p칤xeles sin tener en cuenta la relaci칩n de aspecto\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # actualizar las listas de datos y etiquetas, respectivamente\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "print(\"labels: \", np.unique(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18269832",
      "metadata": {
        "id": "18269832"
      },
      "outputs": [],
      "source": [
        "# convierta los datos y las etiquetas en matrices NumPy mientras escala el p칤xel\n",
        "# intensidades en el rango [0, 255]\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7683250",
      "metadata": {
        "id": "d7683250"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(data[labels=='covid'][1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c3fa55",
      "metadata": {
        "id": "76c3fa55"
      },
      "outputs": [],
      "source": [
        "# realizar una codificaci칩n one-hot en las etiquetas\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9830a0c",
      "metadata": {
        "id": "f9830a0c"
      },
      "outputs": [],
      "source": [
        "# particione los datos en divisiones de entrenamiento y prueba usando el 80% de\n",
        "# los datos para entrenamiento y el 20% restante para pruebas\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240bb995",
      "metadata": {
        "id": "240bb995"
      },
      "source": [
        "## Data Augmentation com mais 3 etapas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1df91c",
      "metadata": {
        "id": "9b1df91c"
      },
      "outputs": [],
      "source": [
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.8, 1.2),  # adicionei: Aumento de brilho\n",
        "    channel_shift_range=10.0,  # adicionei: Mudan칞a de cor\n",
        "    fill_mode='nearest')  # adicionei: Preenchimento de bordas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d632abd",
      "metadata": {
        "id": "9d632abd"
      },
      "outputs": [],
      "source": [
        "# cargue la red VGG16, asegur치ndose de que queden los conjuntos de capas FC principales\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "#headModel = MaxPooling2D(pool_size=(2, 2))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)  # Replace 'decay' with 'learning_rate'\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2abcea",
      "metadata": {
        "id": "9d2abcea"
      },
      "outputs": [],
      "source": [
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "f1 = baseModel.layers[1].output\n",
        "f2 = baseModel.layers[2].output\n",
        "f3 = baseModel.layers[4].output\n",
        "f4 = baseModel.layers[5].output\n",
        "feature_maps = Model(inputs=baseModel.input, outputs=[f1, f2, f3, f4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96333cad",
      "metadata": {
        "id": "96333cad"
      },
      "outputs": [],
      "source": [
        "y = np.argmax(trainY, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d54d9ab",
      "metadata": {
        "id": "5d54d9ab"
      },
      "outputs": [],
      "source": [
        "feat1, feat2, feat3, feat4 = feature_maps.predict(trainX[y==1][0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d222501",
      "metadata": {
        "id": "1d222501"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axs[0, 0].imshow(feat1[0, :, :, 0:3])\n",
        "axs[0, 1].imshow(feat2[0, :, :, 6:9])\n",
        "axs[1, 0].imshow(feat3[0, :, :, 0:3])\n",
        "axs[1, 1].imshow(feat4[0, :, :, 3:6])\n",
        "# plt.subplots\n",
        "# plt.imshow(feat1[0, :, :, 0:3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0438d67",
      "metadata": {
        "id": "e0438d67"
      },
      "source": [
        "##passo os dados de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be60473",
      "metadata": {
        "id": "5be60473"
      },
      "outputs": [],
      "source": [
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375759e8",
      "metadata": {
        "id": "375759e8"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "# plt.savefig(args[\"plot\"])\n",
        "\n",
        "# serialize the model to disk\n",
        "# print(\"[INFO] saving COVID-19 detector model...\")\n",
        "# model.save(args[\"model\"], save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d9b50a",
      "metadata": {
        "id": "74d9b50a"
      },
      "source": [
        "##Teste com VGG19 e compara칞칚o com VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1246ca",
      "metadata": {
        "id": "ed1246ca"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "# Carregar a VGG19 com pesos do ImageNet\n",
        "baseModel_vgg19 = VGG19(weights=\"imagenet\", include_top=False,\n",
        "                        input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# Cabe칞a da rede\n",
        "headModel = baseModel_vgg19.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# Modelo completo\n",
        "model_vgg19 = Model(inputs=baseModel_vgg19.input, outputs=headModel)\n",
        "\n",
        "# Congelar camadas convolucionais\n",
        "for layer in baseModel_vgg19.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compilar\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model_vgg19.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Treinar\n",
        "H19 = model_vgg19.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a879a0f2",
      "metadata": {
        "id": "a879a0f2"
      },
      "outputs": [],
      "source": [
        "# Avalia칞칚o VGG19\n",
        "predIdxs19 = model_vgg19.predict(testX, batch_size=BS)\n",
        "predIdxs19 = np.argmax(predIdxs19, axis=1)\n",
        "\n",
        "print(\"[INFO] Classification report VGG19:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs19, target_names=lb.classes_))\n",
        "\n",
        "cm19 = confusion_matrix(testY.argmax(axis=1), predIdxs19)\n",
        "acc19 = (cm19[0, 0] + cm19[1, 1]) / np.sum(cm19)\n",
        "sens19 = cm19[0, 0] / (cm19[0, 0] + cm19[0, 1])\n",
        "spec19 = cm19[1, 1] / (cm19[1, 0] + cm19[1, 1])\n",
        "\n",
        "print(\"Matriz de Confus칚o VGG19:\")\n",
        "print(cm19)\n",
        "print(\"Acur치cia: {:.4f}\".format(acc19))\n",
        "print(\"Sensibilidade: {:.4f}\".format(sens19))\n",
        "print(\"Especificidade: {:.4f}\".format(spec19))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparacao entre VGG16 e VGG19\n",
        "\n",
        "# make predictions on the testing set for VGG16\n",
        "print(\"[INFO] evaluating VGG16 network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report for VGG16\n",
        "print(\"[INFO] Classification report VGG16:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity for VGG16\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc16 = (cm[0, 0] + cm[1, 1]) / total\n",
        "sens16 = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "spec16 = cm[1, 1] / (cm[1, 0] + cm[0, 1])\n",
        "\n",
        "print(\"Matriz de Confus칚o VGG16:\")\n",
        "print(cm)\n",
        "print(\"Acur치cia VGG16: {:.4f}\".format(acc16))\n",
        "print(\"Sensibilidade VGG16: {:.4f}\".format(sens16))\n",
        "print(\"Especificidade VGG16: {:.4f}\".format(spec16))\n",
        "\n",
        "# Avalia칞칚o VGG19 (assuming model_vgg19, testX, testY, lb are defined in previous cells)\n",
        "print(\"[INFO] evaluating VGG19 network...\")\n",
        "predIdxs19 = model_vgg19.predict(testX, batch_size=BS)\n",
        "predIdxs19 = np.argmax(predIdxs19, axis=1)\n",
        "\n",
        "print(\"[INFO] Classification report VGG19:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs19, target_names=lb.classes_))\n",
        "\n",
        "cm19 = confusion_matrix(testY.argmax(axis=1), predIdxs19)\n",
        "acc19 = (cm19[0, 0] + cm19[1, 1]) / np.sum(cm19)\n",
        "sens19 = cm19[0, 0] / (cm19[0, 0] + cm19[0, 1])\n",
        "spec19 = cm19[1, 1] / (cm19[1, 0] + cm19[1, 1])\n",
        "\n",
        "print(\"Matriz de Confus칚o VGG19:\")\n",
        "print(cm19)\n",
        "print(\"Acur치cia VGG19: {:.4f}\".format(acc19))\n",
        "print(\"Sensibilidade VGG19: {:.4f}\".format(sens19))\n",
        "print(\"Especificidade VGG19: {:.4f}\".format(spec19))\n",
        "\n",
        "\n",
        "# Compara칞칚o final\n",
        "print(\"\\n\\n游댌 Compara칞칚o entre VGG16 e VGG19\")\n",
        "print(\"Modelo\\t\\tAcur치cia\\tSensibilidade\\tEspecificidade\")\n",
        "print(f\"VGG16\\t\\t{acc16:.4f}\\t\\t{sens16:.4f}\\t\\t{spec16:.4f}\")\n",
        "print(f\"VGG19\\t\\t{acc19:.4f}\\t\\t{sens19:.4f}\\t\\t{spec19:.4f}\")\n",
        "\n",
        "# The plotting code for VGG16 training history is removed from here\n",
        "# as it was part of the previous evaluation cell for VGG16 and not\n",
        "# needed for the final comparison table. If you want to plot the\n",
        "# training history for both, you would do it in separate cells after\n",
        "# training each model."
      ],
      "metadata": {
        "id": "e37w0L1uRw6r"
      },
      "id": "e37w0L1uRw6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compara칞칚o entre os modelos VGG16 e VGG19\n",
        "\n",
        "##Acur치cia geral:\n",
        "\n",
        "VGG16 atingiu uma acur치cia de 50%\n",
        "\n",
        "VGG19 obteve 70%, demonstrando melhor desempenho geral na classifica칞칚o\n",
        "\n",
        "##Sensibilidade (recall da classe COVID):\n",
        "\n",
        "Ambos os modelos apresentaram sensibilidade igual (0.4000) para a classe COVID, ou seja, acertaram apenas 2 de 5 imagens positivas\n",
        "\n",
        "##Especificidade (recall da classe normal):\n",
        "\n",
        "O VGG16 teve uma especificidade de 60%\n",
        "\n",
        "O VGG19 alcan칞ou 100%, identificando corretamente todos os casos normais\n",
        "\n",
        "##Conclus칚o:\n",
        "\n",
        "Apesar da sensibilidade baixa em ambos os modelos, o VGG19 se destacou por apresentar acur치cia e especificidade superiores.\n",
        "\n",
        "Isso indica que o VGG19 teve melhor desempenho na identifica칞칚o de imagens normais e foi mais confi치vel no contexto geral da classifica칞칚o, mesmo com um conjunto pequeno de teste."
      ],
      "metadata": {
        "id": "TIrZsez7SOp2"
      },
      "id": "TIrZsez7SOp2"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}