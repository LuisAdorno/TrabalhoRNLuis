{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisAdorno/TrabalhoRNLuis/blob/main/Luis_TrabalhoRN1FinalVGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce7f294",
      "metadata": {
        "id": "4ce7f294"
      },
      "source": [
        "#Vamos Classificar RX de covid e normais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88c99b7",
      "metadata": {
        "id": "b88c99b7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c73e9a1",
      "metadata": {
        "id": "5c73e9a1"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fd750e",
      "metadata": {
        "id": "69fd750e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from imutils import paths"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc2ff74",
      "metadata": {
        "id": "2dc2ff74"
      },
      "source": [
        "Vamos a procurar o caminho de nossa base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5532b06a",
      "metadata": {
        "id": "5532b06a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "workdir_path = '/content/drive/My Drive'\n",
        "os.chdir(workdir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19a8dd4",
      "metadata": {
        "id": "e19a8dd4"
      },
      "source": [
        "#Vamos colocar os parametros de nosso modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59313b17",
      "metadata": {
        "id": "59313b17"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 1e-3\n",
        "EPOCHS = 15 # PODE MUDAR\n",
        "BS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9390f307",
      "metadata": {
        "id": "9390f307"
      },
      "source": [
        "# Pega a lista de imagens no nosso diretório de conjunto de dados e, em seguida, inicializa\n",
        "# a lista de dados e as respectivas classes das imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd0405b8",
      "metadata": {
        "id": "dd0405b8"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_path = 'dataset'\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset_path))\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c7e6fa",
      "metadata": {
        "id": "64c7e6fa"
      },
      "outputs": [],
      "source": [
        "# recorrer las rutas de la imagen\n",
        "for imagePath in imagePaths:\n",
        "    # extrae la etiqueta de clase del nombre del archivo\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "   # cargue la imagen, intercambie canales de color y cambie su tamaño para que sea una\n",
        "   # 224x224 píxeles sin tener en cuenta la relación de aspecto\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # actualizar las listas de datos y etiquetas, respectivamente\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "print(\"labels: \", np.unique(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c4d1a4",
      "metadata": {
        "id": "d5c4d1a4"
      },
      "outputs": [],
      "source": [
        "# convierta los datos y las etiquetas en matrices NumPy mientras escala el píxel\n",
        "# intensidades en el rango [0, 255]\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381406a8",
      "metadata": {
        "id": "381406a8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(data[labels=='covid'][1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd1cdb5",
      "metadata": {
        "id": "3dd1cdb5"
      },
      "outputs": [],
      "source": [
        "# realizar una codificación one-hot en las etiquetas\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf65ed4",
      "metadata": {
        "id": "acf65ed4"
      },
      "outputs": [],
      "source": [
        "# particione los datos en divisiones de entrenamiento y prueba usando el 80% de\n",
        "# los datos para entrenamiento y el 20% restante para pruebas\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d33fec",
      "metadata": {
        "id": "d1d33fec"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ccc77f",
      "metadata": {
        "id": "a4ccc77f"
      },
      "outputs": [],
      "source": [
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bed6fc7",
      "metadata": {
        "id": "2bed6fc7"
      },
      "outputs": [],
      "source": [
        "# cargue la red VGG16, asegurándose de que queden los conjuntos de capas FC principales\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "#headModel = MaxPooling2D(pool_size=(2, 2))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)  # Replace 'decay' with 'learning_rate'\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3549a953",
      "metadata": {
        "id": "3549a953"
      },
      "outputs": [],
      "source": [
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "f1 = baseModel.layers[1].output\n",
        "f2 = baseModel.layers[2].output\n",
        "f3 = baseModel.layers[4].output\n",
        "f4 = baseModel.layers[5].output\n",
        "feature_maps = Model(inputs=baseModel.input, outputs=[f1, f2, f3, f4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386e0316",
      "metadata": {
        "id": "386e0316"
      },
      "outputs": [],
      "source": [
        "y = np.argmax(trainY, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e49b27e",
      "metadata": {
        "id": "2e49b27e"
      },
      "outputs": [],
      "source": [
        "feat1, feat2, feat3, feat4 = feature_maps.predict(trainX[y==1][0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40837031",
      "metadata": {
        "id": "40837031"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axs[0, 0].imshow(feat1[0, :, :, 0:3])\n",
        "axs[0, 1].imshow(feat2[0, :, :, 6:9])\n",
        "axs[1, 0].imshow(feat3[0, :, :, 0:3])\n",
        "axs[1, 1].imshow(feat4[0, :, :, 3:6])\n",
        "# plt.subplots\n",
        "# plt.imshow(feat1[0, :, :, 0:3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd8daf4",
      "metadata": {
        "id": "4dd8daf4"
      },
      "source": [
        "##passo os dados de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6f1a1c",
      "metadata": {
        "id": "3e6f1a1c"
      },
      "outputs": [],
      "source": [
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79a9187",
      "metadata": {
        "id": "d79a9187"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "# plt.savefig(args[\"plot\"])\n",
        "\n",
        "# serialize the model to disk\n",
        "# print(\"[INFO] saving COVID-19 detector model...\")\n",
        "# model.save(args[\"model\"], save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "166926f6",
      "metadata": {
        "id": "166926f6"
      },
      "source": [
        "##Versão modificada com 20 EPOCHS e mais 3 técnicas extras de Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Análise e Conclusão:\n",
        "O modelo com 15 épocas e augmentação básica parece ter tido desempenho perfeito, mas é possível que ele tenha sofrido overfitting, aprendendo demais os dados de treino e acertando por sorte em um conjunto pequeno de teste.\n",
        "\n",
        "Já o modelo com 20 epochs e augmentação mais complexa teve desempenho significativamente inferior. A acurácia caiu para 50%, e tanto a sensibilidade quanto a especificidade foram medianas. Isso pode ter ocorrido por:\n",
        "\n",
        "Excesso de transformações visuais, dificultando o aprendizado\n",
        "\n",
        "Aumento da complexidade sem aumento correspondente no tamanho ou diversidade do dataset\n",
        "\n",
        "Mais epochs sem regularização suficiente, causando instabilidade"
      ],
      "metadata": {
        "id": "qzHOBMVFTqoB"
      },
      "id": "qzHOBMVFTqoB"
    },
    {
      "cell_type": "markdown",
      "id": "05d355c9",
      "metadata": {
        "id": "05d355c9"
      },
      "source": [
        "Vamos a procurar o caminho de nossa base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e81df21",
      "metadata": {
        "id": "9e81df21"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "workdir_path = '/content/drive/My Drive'  # Inserir o local da pasta onde estão os arquivos de entrada (treino e teste)\n",
        "os.chdir(workdir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f590cde1",
      "metadata": {
        "id": "f590cde1"
      },
      "source": [
        "#Vamos colocar os parametros de nosso modelo (20 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7693b9d9",
      "metadata": {
        "id": "7693b9d9"
      },
      "outputs": [],
      "source": [
        "INIT_LR = 1e-3\n",
        "EPOCHS = 20 # Alterado para 20 epochs\n",
        "BS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe7168d",
      "metadata": {
        "id": "cbe7168d"
      },
      "source": [
        "# Pega a lista de imagens no nosso diretório de conjunto de dados e, em seguida, inicializa\n",
        "# a lista de dados e as respectivas classes das imagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5e1885",
      "metadata": {
        "id": "dc5e1885"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_path = 'dataset'\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(dataset_path))\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fdc6a1",
      "metadata": {
        "id": "66fdc6a1"
      },
      "outputs": [],
      "source": [
        "# recorrer las rutas de la imagen\n",
        "for imagePath in imagePaths:\n",
        "    # extrae la etiqueta de clase del nombre del archivo\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "   # cargue la imagen, intercambie canales de color y cambie su tamaño para que sea una\n",
        "   # 224x224 píxeles sin tener en cuenta la relación de aspecto\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # actualizar las listas de datos y etiquetas, respectivamente\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "print(\"labels: \", np.unique(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18269832",
      "metadata": {
        "id": "18269832"
      },
      "outputs": [],
      "source": [
        "# convierta los datos y las etiquetas en matrices NumPy mientras escala el píxel\n",
        "# intensidades en el rango [0, 255]\n",
        "data = np.array(data) / 255.0\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7683250",
      "metadata": {
        "id": "d7683250"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(data[labels=='covid'][1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c3fa55",
      "metadata": {
        "id": "76c3fa55"
      },
      "outputs": [],
      "source": [
        "# realizar una codificación one-hot en las etiquetas\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9830a0c",
      "metadata": {
        "id": "f9830a0c"
      },
      "outputs": [],
      "source": [
        "# particione los datos en divisiones de entrenamiento y prueba usando el 80% de\n",
        "# los datos para entrenamiento y el 20% restante para pruebas\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "                                                  test_size=0.20, stratify=labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240bb995",
      "metadata": {
        "id": "240bb995"
      },
      "source": [
        "## Data Augmentation com mais 3 etapas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1df91c",
      "metadata": {
        "id": "9b1df91c"
      },
      "outputs": [],
      "source": [
        "# initialize the training data augmentation object\n",
        "trainAug = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.8, 1.2),  # adicionei: Aumento de brilho\n",
        "    channel_shift_range=10.0,  # adicionei: Mudança de cor\n",
        "    fill_mode='nearest')  # adicionei: Preenchimento de bordas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d632abd",
      "metadata": {
        "id": "9d632abd"
      },
      "outputs": [],
      "source": [
        "# cargue la red VGG16, asegurándose de que queden los conjuntos de capas FC principales\n",
        "# off\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "#headModel = MaxPooling2D(pool_size=(2, 2))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)  # Replace 'decay' with 'learning_rate'\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2abcea",
      "metadata": {
        "id": "9d2abcea"
      },
      "outputs": [],
      "source": [
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "                  input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "f1 = baseModel.layers[1].output\n",
        "f2 = baseModel.layers[2].output\n",
        "f3 = baseModel.layers[4].output\n",
        "f4 = baseModel.layers[5].output\n",
        "feature_maps = Model(inputs=baseModel.input, outputs=[f1, f2, f3, f4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96333cad",
      "metadata": {
        "id": "96333cad"
      },
      "outputs": [],
      "source": [
        "y = np.argmax(trainY, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d54d9ab",
      "metadata": {
        "id": "5d54d9ab"
      },
      "outputs": [],
      "source": [
        "feat1, feat2, feat3, feat4 = feature_maps.predict(trainX[y==1][0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d222501",
      "metadata": {
        "id": "1d222501"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "axs[0, 0].imshow(feat1[0, :, :, 0:3])\n",
        "axs[0, 1].imshow(feat2[0, :, :, 6:9])\n",
        "axs[1, 0].imshow(feat3[0, :, :, 0:3])\n",
        "axs[1, 1].imshow(feat4[0, :, :, 3:6])\n",
        "# plt.subplots\n",
        "# plt.imshow(feat1[0, :, :, 0:3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0438d67",
      "metadata": {
        "id": "e0438d67"
      },
      "source": [
        "##passo os dados de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be60473",
      "metadata": {
        "id": "5be60473"
      },
      "outputs": [],
      "source": [
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "375759e8",
      "metadata": {
        "id": "375759e8"
      },
      "outputs": [],
      "source": [
        "# make predictions on the testing set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc = (cm[0, 0] + cm[1, 1]) / total\n",
        "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
        "\n",
        "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
        "print(cm)\n",
        "print(\"acc: {:.4f}\".format(acc))\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "# plt.savefig(args[\"plot\"])\n",
        "\n",
        "# serialize the model to disk\n",
        "# print(\"[INFO] saving COVID-19 detector model...\")\n",
        "# model.save(args[\"model\"], save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d9b50a",
      "metadata": {
        "id": "74d9b50a"
      },
      "source": [
        "##Teste com VGG19 e comparação com VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1246ca",
      "metadata": {
        "id": "ed1246ca"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "# Carregar a VGG19 com pesos do ImageNet\n",
        "baseModel_vgg19 = VGG19(weights=\"imagenet\", include_top=False,\n",
        "                        input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# Cabeça da rede\n",
        "headModel = baseModel_vgg19.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.3)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# Modelo completo\n",
        "model_vgg19 = Model(inputs=baseModel_vgg19.input, outputs=headModel)\n",
        "\n",
        "# Congelar camadas convolucionais\n",
        "for layer in baseModel_vgg19.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compilar\n",
        "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model_vgg19.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "# Treinar\n",
        "H19 = model_vgg19.fit(\n",
        "    trainAug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a879a0f2",
      "metadata": {
        "id": "a879a0f2"
      },
      "outputs": [],
      "source": [
        "# Avaliação VGG19\n",
        "predIdxs19 = model_vgg19.predict(testX, batch_size=BS)\n",
        "predIdxs19 = np.argmax(predIdxs19, axis=1)\n",
        "\n",
        "print(\"[INFO] Classification report VGG19:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs19, target_names=lb.classes_))\n",
        "\n",
        "cm19 = confusion_matrix(testY.argmax(axis=1), predIdxs19)\n",
        "acc19 = (cm19[0, 0] + cm19[1, 1]) / np.sum(cm19)\n",
        "sens19 = cm19[0, 0] / (cm19[0, 0] + cm19[0, 1])\n",
        "spec19 = cm19[1, 1] / (cm19[1, 0] + cm19[1, 1])\n",
        "\n",
        "print(\"Matriz de Confusão VGG19:\")\n",
        "print(cm19)\n",
        "print(\"Acurácia: {:.4f}\".format(acc19))\n",
        "print(\"Sensibilidade: {:.4f}\".format(sens19))\n",
        "print(\"Especificidade: {:.4f}\".format(spec19))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparacao entre VGG16 e VGG19\n",
        "\n",
        "# make predictions on the testing set for VGG16\n",
        "print(\"[INFO] evaluating VGG16 network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report for VGG16\n",
        "print(\"[INFO] Classification report VGG16:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "                            target_names=lb.classes_))\n",
        "\n",
        "# compute the confusion matrix and and use it to derive the raw\n",
        "# accuracy, sensitivity, and specificity for VGG16\n",
        "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
        "total = sum(sum(cm))\n",
        "acc16 = (cm[0, 0] + cm[1, 1]) / total\n",
        "sens16 = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
        "spec16 = cm[1, 1] / (cm[1, 0] + cm[0, 1])\n",
        "\n",
        "print(\"Matriz de Confusão VGG16:\")\n",
        "print(cm)\n",
        "print(\"Acurácia VGG16: {:.4f}\".format(acc16))\n",
        "print(\"Sensibilidade VGG16: {:.4f}\".format(sens16))\n",
        "print(\"Especificidade VGG16: {:.4f}\".format(spec16))\n",
        "\n",
        "# Avaliação VGG19 (assuming model_vgg19, testX, testY, lb are defined in previous cells)\n",
        "print(\"[INFO] evaluating VGG19 network...\")\n",
        "predIdxs19 = model_vgg19.predict(testX, batch_size=BS)\n",
        "predIdxs19 = np.argmax(predIdxs19, axis=1)\n",
        "\n",
        "print(\"[INFO] Classification report VGG19:\")\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs19, target_names=lb.classes_))\n",
        "\n",
        "cm19 = confusion_matrix(testY.argmax(axis=1), predIdxs19)\n",
        "acc19 = (cm19[0, 0] + cm19[1, 1]) / np.sum(cm19)\n",
        "sens19 = cm19[0, 0] / (cm19[0, 0] + cm19[0, 1])\n",
        "spec19 = cm19[1, 1] / (cm19[1, 0] + cm19[1, 1])\n",
        "\n",
        "print(\"Matriz de Confusão VGG19:\")\n",
        "print(cm19)\n",
        "print(\"Acurácia VGG19: {:.4f}\".format(acc19))\n",
        "print(\"Sensibilidade VGG19: {:.4f}\".format(sens19))\n",
        "print(\"Especificidade VGG19: {:.4f}\".format(spec19))\n",
        "\n",
        "\n",
        "# Comparação final\n",
        "print(\"\\n\\n🔍 Comparação entre VGG16 e VGG19\")\n",
        "print(\"Modelo\\t\\tAcurácia\\tSensibilidade\\tEspecificidade\")\n",
        "print(f\"VGG16\\t\\t{acc16:.4f}\\t\\t{sens16:.4f}\\t\\t{spec16:.4f}\")\n",
        "print(f\"VGG19\\t\\t{acc19:.4f}\\t\\t{sens19:.4f}\\t\\t{spec19:.4f}\")\n",
        "\n",
        "# The plotting code for VGG16 training history is removed from here\n",
        "# as it was part of the previous evaluation cell for VGG16 and not\n",
        "# needed for the final comparison table. If you want to plot the\n",
        "# training history for both, you would do it in separate cells after\n",
        "# training each model."
      ],
      "metadata": {
        "id": "e37w0L1uRw6r"
      },
      "id": "e37w0L1uRw6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparação entre os modelos VGG16 e VGG19\n",
        "\n",
        "##Acurácia geral:\n",
        "\n",
        "VGG16 atingiu uma acurácia de 50%\n",
        "\n",
        "VGG19 obteve 70%, demonstrando melhor desempenho geral na classificação\n",
        "\n",
        "##Sensibilidade (recall da classe COVID):\n",
        "\n",
        "Ambos os modelos apresentaram sensibilidade igual (0.4000) para a classe COVID, ou seja, acertaram apenas 2 de 5 imagens positivas\n",
        "\n",
        "##Especificidade (recall da classe normal):\n",
        "\n",
        "O VGG16 teve uma especificidade de 60%\n",
        "\n",
        "O VGG19 alcançou 100%, identificando corretamente todos os casos normais\n",
        "\n",
        "##Conclusão:\n",
        "\n",
        "Apesar da sensibilidade baixa em ambos os modelos, o VGG19 se destacou por apresentar acurácia e especificidade superiores.\n",
        "\n",
        "Isso indica que o VGG19 teve melhor desempenho na identificação de imagens normais e foi mais confiável no contexto geral da classificação, mesmo com um conjunto pequeno de teste."
      ],
      "metadata": {
        "id": "TIrZsez7SOp2"
      },
      "id": "TIrZsez7SOp2"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}